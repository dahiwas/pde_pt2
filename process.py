from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, input_file_name, regexp_extract, col
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import os
import glob
import time

def process_files(file_path: str):
    start_total_time = time.time()
    
    # ğŸš€ CONFIGURAÃ‡ÃƒO ULTRA-OTIMIZADA PARA 4 CORES
    spark = SparkSession.builder \
        .appName("ClickHouse Ultra-Fast - 4Core Max Performance") \
        .config("spark.jars", "clickhouse-jdbc-0.3.2-patch9-all.jar") \
        .config("spark.master", "local[4]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64m") \
        .config("spark.sql.shuffle.partitions", "16") \
        .config("spark.executor.memory", "4g") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.cores", "4") \
        .config("spark.sql.files.maxPartitionBytes", "128m") \
        .config("spark.default.parallelism", "16") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.sql.files.openCostInBytes", "1048576") \
        .config("spark.executor.instances", "1") \
        .config("spark.dynamicAllocation.enabled", "false") \
        .config("spark.sql.adaptive.localShuffleReader.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "64m") \
        .config("spark.sql.files.ignoreCorruptFiles", "false") \
        .config("spark.sql.files.ignoreMissingFiles", "false") \
        .config("spark.sql.parquet.columnarReaderBatchSize", "8192") \
        .config("spark.sql.inMemoryColumnarStorage.batchSize", "20000") \
        .getOrCreate()

    # ğŸ”¥ CONFIGURAÃ‡Ã•ES CLICKHOUSE ULTRA-OTIMIZADAS
    url = "jdbc:clickhouse://clickhouse:8123/default"
    properties = {
        "driver": "com.clickhouse.jdbc.ClickHouseDriver",
        "user": "default",
        "password": "genetic123",
        # Timeouts otimizados
        "socket_timeout": "3600000",  # 1 hora
        "connection_timeout": "300000",  # 5 minutos
        "max_execution_time": "3600",  # 1 hora
        # CompressÃ£o e buffers otimizados
        "compress": "1",
        "decompress": "1",
        "compression": "lz4",
        "buffer_size": "1048576",  # 1MB
        "max_buffer_size": "10485760",  # 10MB
        # Performance crÃ­tica
        "max_threads": "4",
        "max_memory_usage": "4000000000",  # 4GB
        "max_bytes_before_external_group_by": "2000000000",  # 2GB
        "max_bytes_before_external_sort": "2000000000",  # 2GB
        # InserÃ§Ã£o otimizada
        "max_insert_block_size": "1000000",  # 1M rows
        "min_insert_block_size_rows": "500000",  # 500K rows
        "min_insert_block_size_bytes": "268435456",  # 256MB
        "max_insert_threads": "4",
        # ConfiguraÃ§Ãµes de rede
        "send_progress_in_http_headers": "1",
        "http_connection_timeout": "300000",
        "http_send_timeout": "3600000",
        "http_receive_timeout": "3600000",
        # OtimizaÃ§Ãµes especÃ­ficas
        "async_insert": "0",  # InserÃ§Ã£o sÃ­ncrona para controle total
        "wait_for_async_insert": "0",
        "optimize_on_insert": "0",  # NÃ£o otimizar durante inserÃ§Ã£o
        "fsync_metadata": "0",  # NÃ£o forÃ§ar sync de metadata
        "ssl": "false"
    }

    print("ğŸš€ MODO ULTRA-PERFORMANCE - 4 CORES MÃXIMA VELOCIDADE")
    print(f"ğŸ’» ConfiguraÃ§Ã£o: 4 cores ativos, inserÃ§Ã£o em lotes de 1M registros")
    
    # Schema otimizado
    schema = StructType([
        StructField("snp_id", StringType(), False),
        StructField("chromosome", StringType(), False), 
        StructField("position", StringType(), False),
        StructField("genotype", StringType(), False)
    ])

    # Descobrir arquivos
    samples_dir = ""
    csv_pattern = os.path.join(samples_dir, "*.csv")
    csv_files = glob.glob(csv_pattern)
    
    total_size_mb = sum(os.path.getsize(f) / (1024 * 1024) for f in csv_files)
    print(f"ğŸ“ {len(csv_files)} arquivos, {total_size_mb:.1f} MB total")
    
    if not csv_files:
        print("âŒ Nenhum arquivo encontrado")
        spark.stop()
        return

    print("âš¡ LEITURA ULTRA-RÃPIDA EM 4 CORES...")
    read_start = time.time()
    
    # Leitura otimizada para mÃ¡xima velocidade
    df = spark.read \
        .option("sep", "\t") \
        .option("header", "false") \
        .option("comment", "#") \
        .option("mode", "FAILFAST") \
        .option("multiline", "false") \
        .option("escape", "") \
        .option("quote", "") \
        .option("ignoreLeadingWhiteSpace", "false") \
        .option("ignoreTrailingWhiteSpace", "false") \
        .option("maxCharsPerColumn", "1000") \
        .schema(schema) \
        .csv(csv_pattern)
    
    read_time = time.time() - read_start
    print(f"âœ… Leitura: {read_time:.1f}s")

    print("âš¡ TRANSFORMAÃ‡Ã•ES ULTRA-RÃPIDAS...")
    transform_start = time.time()
    
    # Pipeline ultra-otimizado
    df = df.withColumn("filename", input_file_name()) \
        .withColumn("individual_id", regexp_extract("filename", r"([^/]+)\.csv$", 1)) \
        .withColumn("position", col("position").cast(IntegerType())) \
        .select("individual_id", "snp_id", "chromosome", "position", "genotype") \
        .filter(col("individual_id") != "")
    
    # Reparticionamento otimizado para 4 cores (16 partiÃ§Ãµes = 4 por core)
    df = df.repartition(16, "individual_id")
    
    transform_time = time.time() - transform_start
    print(f"âœ… TransformaÃ§Ãµes: {transform_time:.1f}s")

    print("âš¡ CONTAGEM ULTRA-RÃPIDA...")
    count_start = time.time()
    df.cache()
    total_records = df.count()
    count_time = time.time() - count_start
    print(f"âœ… {total_records:,} registros em {count_time:.1f}s")

    print("ğŸ”¥ INSERÃ‡ÃƒO ULTRA-RÃPIDA EM 4 CORES - LOTES DE 1M...")
    insert_start = time.time()
    
    try:
        # InserÃ§Ã£o ultra-otimizada com lotes gigantes
        df.write \
            .mode("append") \
            .option("batchsize", "1000000") \
            .option("numPartitions", "16") \
            .option("isolationLevel", "NONE") \
            .option("truncate", "false") \
            .option("rewriteBatchedStatements", "true") \
            .option("queryTimeout", "3600") \
            .option("loginTimeout", "300") \
            .jdbc(url=url, table="genotypes_raw", properties=properties)
        
        insert_time = time.time() - insert_start
        print(f"âœ… InserÃ§Ã£o: {insert_time:.1f}s")
        
        # VerificaÃ§Ã£o final
        print("âš¡ VerificaÃ§Ã£o...")
        verify_start = time.time()
        final_count = spark.read.jdbc(url=url, table="genotypes_raw", properties=properties).count()
        verify_time = time.time() - verify_start
        
        total_time = time.time() - start_total_time
        
        print(f"\nğŸ‰ PROCESSAMENTO ULTRA-RÃPIDO CONCLUÃDO!")
        print(f"ğŸ“Š ESTATÃSTICAS FINAIS:")
        print(f"   âš¡ Tempo total: {total_time:.1f}s ({total_time/60:.1f} minutos)")
        print(f"   ğŸ“ˆ Registros processados: {total_records:,}")
        print(f"   ğŸš€ Taxa: {total_records/total_time:,.0f} registros/segundo")
        print(f"   ğŸ’¾ Total na tabela: {final_count:,}")
        print(f"   ğŸ“ Throughput: {total_size_mb/total_time:.1f} MB/s")
        print(f"   ğŸ’» UtilizaÃ§Ã£o: 4/4 cores (100%)")
        
        # AnÃ¡lise de performance
        expected_min_rate = 50000  # 50K registros/segundo mÃ­nimo esperado
        if total_records/total_time >= expected_min_rate:
            print(f"ğŸ† EXCELENTE! Taxa acima de {expected_min_rate:,} registros/segundo!")
        else:
            print(f"âš ï¸  Taxa abaixo do esperado ({expected_min_rate:,} registros/segundo)")
        
        # Breakdown detalhado
        print(f"\nâ±ï¸ BREAKDOWN DE TEMPOS:")
        print(f"   ğŸ“– Leitura: {read_time:.1f}s ({read_time/total_time*100:.1f}%)")
        print(f"   ğŸ”„ TransformaÃ§Ãµes: {transform_time:.1f}s ({transform_time/total_time*100:.1f}%)")
        print(f"   ğŸ”¢ Contagem: {count_time:.1f}s ({count_time/total_time*100:.1f}%)")
        print(f"   ğŸ’¾ InserÃ§Ã£o: {insert_time:.1f}s ({insert_time/total_time*100:.1f}%)")
        print(f"   âœ… VerificaÃ§Ã£o: {verify_time:.1f}s ({verify_time/total_time*100:.1f}%)")
        
        # AnÃ¡lise de eficiÃªncia ultra-detalhada
        cpu_efficiency = (total_records / total_time) / 4  # registros por segundo por core
        print(f"\nğŸ’» EFICIÃŠNCIA ULTRA-DETALHADA:")
        print(f"   ğŸ”¥ {cpu_efficiency:,.0f} registros/segundo/core")
        print(f"   ğŸ“Š {total_size_mb/total_time/4:.1f} MB/s/core")
        print(f"   âš¡ {1000000/cpu_efficiency:.1f} microssegundos/registro/core")
        
        # ComparaÃ§Ã£o com expectativas
        if total_time <= 180:  # 3 minutos
            print(f"ğŸ† ULTRA-RÃPIDO! Processamento em menos de 3 minutos!")
        elif total_time <= 300:  # 5 minutos
            print(f"ğŸš€ RÃPIDO! Processamento em menos de 5 minutos!")
        elif total_time <= 600:  # 10 minutos
            print(f"ğŸ‘ BOM! Dentro do tempo aceitÃ¡vel!")
        else:
            print(f"âš ï¸  LENTO! Precisa de mais otimizaÃ§Ã£o!")
        
        # Dicas de otimizaÃ§Ã£o
        print(f"\nğŸ’¡ ANÃLISE DE PERFORMANCE:")
        if insert_time/total_time > 0.7:
            print(f"   âš ï¸  InserÃ§Ã£o Ã© o gargalo ({insert_time/total_time*100:.1f}% do tempo)")
            print(f"   ğŸ’¡ Considere aumentar batchsize ou otimizar ClickHouse")
        if read_time/total_time > 0.3:
            print(f"   âš ï¸  Leitura estÃ¡ lenta ({read_time/total_time*100:.1f}% do tempo)")
            print(f"   ğŸ’¡ Considere usar formato Parquet ou otimizar I/O")
        
        print(f"\nğŸ”§ MONITORAMENTO RECOMENDADO:")
        print(f"   â€¢ htop: todos os 4 cores devem estar ~100%")
        print(f"   â€¢ iotop: verificar I/O de disco")
        print(f"   â€¢ nethogs: verificar trÃ¡fego de rede para ClickHouse")
        
    except Exception as e:
        print(f"âŒ Erro durante inserÃ§Ã£o: {str(e)}")
        print(f"ğŸ’¡ Dica: Verifique se ClickHouse estÃ¡ configurado para receber lotes grandes")
        raise
    finally:
        df.unpersist()

    spark.stop()
    print("ğŸ SparkSession finalizada - 4 cores liberados") 